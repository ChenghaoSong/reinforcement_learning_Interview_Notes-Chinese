###深度强化学习

- 简述on-policy和off-policy的区别

  - on-policy:行为策略和要优化的策略是一个策略，更新了策略后，就用该策略的最新版本采样数据
  - off-policy：使用任意的一个行为策略来收集收据， 利用收集的数据更新目标策略

- off-policy 和 on-policy的优劣之分

  - on-policy利用已知当前最优选择，可能学不到最优解，而陷入局部最优；而加入探索又降低了学习效率；on-policy快但是局部最优
  - off-policy 先产生某概率分布下的大量行为数据（behavior policy），意在探索。从这些偏离（off）最优策略的数据中寻求target policy。当然这么做是需要满足数学条件的：假設π是目标策略, µ是行为策略，那么从µ学到π的条件是：π(a|s) > 0 必然有 µ(a|s) > 0成立。
  - off-policy劣势是曲折，收敛慢，但优势是更为强大和通用。其强大是因为它确保了数据全面性，所有行为都能覆盖。甚至其数据来源可以多样，自行产生、或者外来数据均可。

- value-based 和 policy-based的区别

  - value-based通过求解最优值函数间接的求解最优策略；policy-based的方法直接将策略参数化， 通过策略搜索，策略梯度或者进化方法来更新策略的参数以最大化回报。
  - 优缺点
    - 基于值函数的方法不易扩展到连续动作空间
    - policy-based的方法在高维和连续动作空间更加有效，可以学习随机的策略，但同样的容易收敛到局部最小值、方差较大。
    - 如果不用Import Sample,policy gradient 每次更新完参数就要重新采样，因为更新完参数，他的policy分布发生变化，而他更新参数使用的数据是按照policy概率分布采样，既然分布变了，原来的数据就不能用了——on-policy

- 不打破数据相关性，神经网络的训练效果为什么就不好？

  - 在神经网络中通常使用随机梯度下降法。随机的意思是我们随机选择一些样本来增量式的估计梯度，比如常用的采用batch训练。如果样本是相关的，那就意味着前后两个batch的很可能也是相关的，那么估计的梯度也会呈现出某种相关性。如果不幸的情况下，后面的梯度估计可能会抵消掉前面的梯度量。从而使得训练难以收敛。

- DQN的loss

  - mse

- DQN的两个关键trick分别是什么？

  - 经验池的作用
    - 去除了数据相关性，并且缓和了数据分布的差异
    - Prioritized DQN
      - sum Tree
        - https://zhuanlan.zhihu.com/p/47578210代码
        - 爸爸是两个孩子节点之和，只有叶子节点存了东西
  - 使用目标网络(target network)来缓解训练不稳定的问题
    - 为了减少action value Q(s,a)和 目标 r+gamma * maxQ(s',a') 之间的相关性

- DDQN 对 DQN 的改进

  - DQN每次使用当前值函数最大的动作值函数来更新当前的动作值函数，max操作会导致过估计
  - 为了解耦这两个过程，double DQN 使用了两个值网络，一个网络用来执行动作选择，然后用另一个值函数对一个的动作值更新当前网络。

- 探索 强化学习中的探索与利用（count-based) - 白辰甲的文章 - 知乎 https://zhuanlan.zhihu.com/p/78273736

  -  epsilon-贪心在其中加入了随机的探索，可以在一定程度上解决问题，但没有衡量状态的不确定性。
  - 虚拟计数: count-based exploration + CTS模型 / Pixel-CNN模型
  - 基于环境模型误差的方法：Intrinsic curiosity model (ICM)
  - 基于随机网络的方法：Random network distillation (RND)
  - 基于随机网络的方法：NoiseNet
  - 例子：在PG方法中加入Regularizaiton with Policy Entropy 

- reward 稀疏

  - reward shaping，上分层学习
  - C51中，考虑价值函数的分布能够缓解奖励稀疏的问题，较为稀疏的奖励会在通常的迭代中慢慢“稀释”
    - 考虑价值函数的分布的好处
  - 增加支线任务

- multi-step 和 one-step相比

  - 方差变大，偏差变小
    - 原因是，蒙特卡洛的期望就是价值函数的定义，因此无偏差，但是完全采样，得到的G_t随机性大，方差无穷大
      - TD根据r+gamma*v(s_(t+1)),因此，是否无偏要根据V_(s_t+1)来，有限步骤的随机状态和动作，所以随机性更小，方差小
  - 优缺点：
    - 优点：能够更快的传播变化，agent 如果经历了一条transition,它的reward是意料之外的，如果1-step，那么它只改变了1个v(s)，而n-step,这个改变将向后传递给n个step，因此，快很多
    - 缺点：方差大

- Distributional RL

  - 价值函数的分布 的 好处
    - 1.分布相比于均值可以提供更多信息，比如对于某些risk aware的场景，我们可能会更倾向于选择方差较小或者最坏情况较好的行动，而不是一味地选择均值较高的行动
    - 2.对于某些简并状态的MDP或者POMDP，表征可能看起来一样的状态，但是具有两个完全不同的值函数，只考虑均值，这部分信息可能被混淆
    - 3.能够缓解奖励稀疏问题，较为稀疏的奖励会在迭代中被稀释
  - Wasserstein距离
  - 

- 从PG开始

  - PG推导

    - PG化简
      - t1<t2，那么，pi(s_t2,a_t2)和 sum_{0~t1}(st,at)无关

  - 确定性和随机性策略关系

    - 随机性策略，∑π(a|s)=1 策略输出的是动作的概率，使用正态分布对动作进行采样选择，即每个动作都有概率被选到；优点，将探索和改进集成到一个策略中；缺点，需要大量训练数据。
    - 确定性策略，π(s)S→A策略输出即是动作；优点，需要采样的数据少，算法效率高；缺点，无法探索环境。然而因为我们引用了DQN的结构利用off Policy采样，这样就解决了无法探索环境的问题

  - DPG

    - PG缺点

      - 直接对轨迹的价值函数求导，on-policy,非常依赖与环境交互的过程，并且无法利用之前采样的数据进行更新策略

    - DQN缺点

      - 在DQN等值函数估计方法中，最终策略是对动作状态值函数取极大，因此，只能用在离散动作空间，无法使用于较大离散空间，或者连续的动作空间

    - DPG是确定性策略，可以在确定的行动上加上噪声作为随机策略

    - 确定性策略公式

      - u是一个确定性函数映射，给定状态和参数，输出的动作是确定的

        ![img](https://img.mubu.com/document_image/cdc2e0bb-2df2-40e9-988c-352b4fd4c14b-726627.jpg)

      - 确定性策略梯度

        - 

          ![img](https://img.mubu.com/document_image/91df99e5-8eb0-48e6-a133-0675db49c7ca-726627.jpg)

    - 目标函数

      - 

        ![img](https://img.mubu.com/document_image/b0de96f8-fa1a-4d7b-a6fc-05f0b6ea02fc-726627.jpg)

    - 梯度更新

      - 

        ![img](https://img.mubu.com/document_image/437e5659-157e-4337-bc2e-791e96d93749-726627.jpg)

    - 公式中Q(s_t+1,a_t+1)怎么确定呢？

      - 通过u(s_t+1)直接确定！

  - DDPG

    - 主要特点

      - 1、DDPG可以看做是Nature DQN、Actor-Critic和DPG三种方法的组合算法。
      - 2、Critic部分的输入为states和action。
      - 3、Actor部分使用DPG的思想进行更新，使用critic部分Q值对action的梯度来对actor进行更新。
      - 4、使用了Nature DQN的思想，加入了经验池、随机抽样和目标网络，real Q值使用两个target网络共同计算。
      - 5、target网络更新改为软更新，在每个batch缓慢更新target网络的参数。
      - 6、 将ε-greedy探索的方法使用在连续值采样上，通过Ornstein-Uhlenbeck process为action添加噪声。

    - 算法过程

      - 

        ![img](https://img.mubu.com/document_image/c8f0b1fe-a157-4f71-9003-4219996b43e8-726627.jpg)

    - 更新公式

      - 注意，target 参数就是，behave net参数*t +(1-t)*原target net参数

        - 

          ![img](https://img.mubu.com/document_image/db30c271-2e4d-43a0-a203-deece8acc622-726627.jpg)

    - 与DQN的关系

      - 结合了DQN中的经验池和target net概念

    - 与AC算法的关系

      - 属于AC算法 https://blog.csdn.net/qq_30615903/article/details/80776715

        - 相当于深度确定性策略梯度+AC（DDPG）

          - 结构图

            - 

              ![img](https://img.mubu.com/document_image/061b873f-602d-4da6-900f-907895b8cd9a-726627.jpg)

        - 个人觉得和AC算法的区别

          - 1.critic 和 actor都有两个网络，目标网络critic网络中的动作来自于目标actor网络，而不是通过argmax来得到
          - 2.actor动作选择上，使用的是确定性策略u(s)，而非随机策略

        - 共同点

          - 都是AC

- 从PG到AC

  - 从PG到AC

    - 将PG中的R(t)改为Q(s_t,a_t)，同时创建一个critic网络来计算Q函数的值，则为AC算法

  - 如何完整的介绍A3C 算法 https://www.cnblogs.com/wangxiaocvpr/p/8110120.html

    - 1.介绍目标函数和参数更新方法

      - PG推导
      - 将G改为Q(s,a)
        - 将Q(s,a)改为V(s)
          - 1.引入Advantage function
          - 2.引入advantage function后，Q(s,a)和V(s)都存在，因此利用贝尔曼公式，将Q(s,a)换成，r+V(s_t+1)
      - 更新方法
      - 如何异步
        - 缺点：它的异步更新方式在每个线程调用梯度更新时会更新所有的共享的参数，所以会存在某个线程覆盖另一个线程参数的情况。

    - Loss 的组成

      - policy loss

        - 

          ![img](https://img.mubu.com/document_image/39fa91c8-f8bd-4c73-948e-5e31221bfb09-726627.jpg)

      - value loss

        - 

          ![img](https://img.mubu.com/document_image/fbac5716-0634-43ba-98e0-06d1bc58fc9c-726627.jpg)

          ![img](https://img.mubu.com/document_image/d6936e3b-733f-4878-bba9-1a1dc2412515-726627.jpg)

          ![img](https://img.mubu.com/document_image/fbcb6070-9dc4-45ff-b6e4-60082f37da9d-726627.jpg)

      - regulation policy entropy

        - 用途

          - 平衡探索和利用

            ![img](https://img.mubu.com/document_image/4cf9079a-2a0e-4786-a98a-300a1c22dcec-726627.jpg)

    - A3C算法过程

      - 

        ![img](https://img.mubu.com/document_image/ea26f8ef-7ebe-43e6-b6ad-8428eb62152d-726627.jpg)

  - Actor-Critic两者的区别是什么？

    - Actor是策略模块，输出动作；critic是判别器，用来计算值函数。

  - actor-critic框架中的critic起了什么作用？

    - critic表示了对于当前决策好坏的衡量。结合策略模块，当critic判别某个动作的选择时有益的，策略就更新参数以增大该动作出现的概率，反之降低 动作出现的概率。

  - A2C 中的advantage function 和 DQN中的advantage function有什么区别

    - 在dqn中，引入advantage function是为了更好的对最优值函数Q(s,a)进行估计，帮助找到最优策略
    - 在AC算法中，advantage function是针对策略pi的，好的策略advantage function >0,反之小于0，从而提升当前策略

  - 广义优势函数估计 GAE是什么？

    - 好处：兼顾偏差的同时，有效地减少了方差

    - 优势函数的2步估计及无穷步估计分别为

    - A1

      ![img](https://img.mubu.com/document_image/fd3c6e22-f5fe-49a5-92e4-6dfc729d6037-726627.jpg)

    - An，n越大，对优势函数的估计的偏差越小，方差越大

      - 

        ![img](https://img.mubu.com/document_image/1e6d4a38-7dc7-48bb-baab-1bdea06a1202-726627.jpg)

    - 随着step的增加，方差和偏差的变化

      - 偏差是由gamma^n * V(s_t+n)引入的，因此n 越大，偏差越微不足道
      - 另一方面，r是通过采样得到的，因此，存在方差

    - 公式

      - GAE(gamma,0)

        - 

          ![img](https://img.mubu.com/document_image/5b937bae-b04a-490c-a81f-f27eacac0b5f-726627.jpg)

      - GAE(gamma,1)

        - 左边的期望就是Q(s,a)

          ![img](https://img.mubu.com/document_image/a0301e5a-bb7d-4404-9310-3392628adf01-726627.jpg)

      - 利用指数加权平均从1步到无穷步的优势函数估计

        ![img](https://img.mubu.com/document_image/c5711aa4-2a0a-4654-8691-636233e867e4-726627.jpg)

      - 用途

        - 利用GAE代替AC方法中的Critic会产生更好的效果

  - A3C是on-policy，为什么？

    - 不同的Agent在机器上并行运行也可以看作是在探索环境的不同部分，同时也可以让不同的Agent采用不同的策略，这样就可以进一步扩大化数据的多样性，这也使得进行在线更新的样本之间的关联度减少。

- 强化学习DQN，DDQN，AC，DDPG的区别https://blog.csdn.net/hjw756517/article/details/83862664

- DDPG是on-policy还是off-policy，为什么？

  - off-policy。因为在DDPG为了保证一定的探索，对于输出动作加了一定的噪音，也就是说行为策略不再是优化的策略。

- DDPG和A3C的区别

  - 1.DDPG有4个网络，而A3C网络，共享基础层，只是输出层会分为value 和 policy两个分支
    - 为什么不共享critic 和 actor的基础层
      - DDPG中的actor和critic网络很难共享参数，因为一个是输入是状态，一个输入是（状态-动作）对。
      - 至于梯度问题，一般只使用critic的梯度更新共享层，因为对于DDPG来说，Q函数基本收敛了policy才能基本收敛。
      - 不使用Actor网络的梯度是因为它会改变共享层给Q函数的"特征表达"，频繁改变会导致模型不稳定。
      - 强化学习DDPG训练时，当actor和critic共享底层网络。如何训练critic? - Keavnn的回答 - 知乎 https://www.zhihu.com/question/344650972/answer/819718796
      - DDPG训练一段时间后出现只输出action bound值怎么办？
  - 2.A3C利用多个agent在多个环境实例中异步执行和学习，不同线程使用不同的探索策略，使得数据关联性很小，而DDPG则是通过经验池来降低关联性
  - 3.DDPG使用的是确定性策略进行动作选取，而A3C还是标准的随机策略
  - 4.DDPG的Actor部分使用DPG的思想进行更新，使用critic部分Q值对action的梯度来对actor进行更新。而A3C则通过PG的思想进行更新
  - 5.A3C 算法没有target net

- D4PG（一定要讲的非常清楚）

  - 

- COMA

- VIN

- VPN

- 

- TRPO和PPO

  - KL散度

    - 从信息量到交叉熵

      - 信息量

        ![img](https://img.mubu.com/document_image/f6116e06-2639-49ee-aa72-a6cd6ebf8b48-726627.jpg)

      - 信息熵

        ![img](https://img.mubu.com/document_image/610665db-cb7c-4865-91c8-d78c88fbb060-726627.jpg)

      - KL散度

        ![img](https://img.mubu.com/document_image/e7cd8e2c-d085-4300-8ce9-9ee5db7db889-726627.jpg)

      - 交叉熵

        ![img](https://img.mubu.com/document_image/0cf708f4-432e-4bc7-b437-9d1d465a4982-726627.jpg)

    - 公式

      ![img](https://img.mubu.com/document_image/e7cd8e2c-d085-4300-8ce9-9ee5db7db889-726627.jpg)

    - 用来衡量两个概率分布之间的相似程度，

    - 其重要性质是非负性，

    - 当且仅当两个概率分布处处相等时，KL散度取到零。

    - KL散度一般不具有对称性。

    - sum(p(x)log(p(x)/q(x)))

  - KL

    - 它是两个概率分布（probability distribution）间差异的非对称性度量

  - 信赖域是什么？

    - 信赖域介绍

      - [https://www.codelast.com/%E5%8E%9F%E5%88%9B%E4%BF%A1%E8%B5%96%E5%9F%9Ftrust-region%E7%AE%97%E6%B3%95%E6%98%AF%E6%80%8E%E4%B9%88%E4%B8%80%E5%9B%9E%E4%BA%8B/](https://www.codelast.com/原创信赖域trust-region算法是怎么一回事/)

    - 二次曲面拟合多n维曲面，若拟合够好，二次曲面在一给定区间[a,b]内的最大值为梯度更新的方向和位移。这一区间[a,b]即为信赖域。

    - 信赖域数学模型

      - 利用二次模型模拟目标函数 f(x) ，再用二次模型计算出位移 s 。根据位移 ss 可以确定下一点 x+s ，从而可以计算出目标函数的下降量（下降是最优化的目标），再根据下降量来决定扩大信赖域或缩小信赖域。

        ![img](https://img.mubu.com/document_image/6ef08bf0-e906-4df2-b6fb-267ef779698b-726627.jpg)

      - hk就是信赖域的半径

      - gk就是梯度，Gk就是海森矩阵

      - 判断缩小扩大信赖域的方法

        - 例子

          - 什么时候缩小，什么时候扩大，什么时候x=x+s，什么时候x=x

            ![img](https://img.mubu.com/document_image/37477805-13ac-4028-9edc-054088f732d1-726627.jpg)

        - 方法

          ![img](https://img.mubu.com/document_image/7a101d01-afe3-4090-99eb-19a0147fce29-726627.jpg)

  - TRPO

    - 目的
      - 策略梯度的硬伤就在于更新步长α ,当步长选的不合适的时候更新的参数会更差，因此很容易导致越学越差，最后崩溃，而TRPO的目的就是解决这个问题
    - 为什么TRPO能保证新策略的回报函数单调不减？（手推公式啊）
    - TRPO是如何通过优化方法使每个局部点找到让损失函数非增的最优步长来解决学习率的问题
    - 为什么用平均KL散度代替最大KL散度
      - 文章中写： overall the result suggests that the average KL divergence constraint has a similar effect as the theorecally justified maximum KL divergence.
    - 简述TRPO算法
    - 手推TRPO

  - PPO

    - 简述PPO算法
    - 手推PPO
    - PPO 和 TRPO的关系
    - 简述DPPO和PPO的关系？