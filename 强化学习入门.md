## 强化学习知识树

- ### 强化学习基础

  - #### 强化学习和监督学习以及非监督学习的区别

    - 监督学习是使用已经标记好的数据样本，做训练来预测新的数据的类型（分类）或者值（回归）
    - 非监督学习是找到unlabeled 数据背后的关系
    - 而强化学习的目标是最大化最大化累计回报

  - #### 概念

    - 强化学习分为两大类——model-free or model-based
    - Exploring Starts 假设 - 指有一个探索起点的环境。
    - 比如：围棋的当前状态就是一个探索起点。自动驾驶的汽车也许是一个没有起点的例子。
    - first-visit - 在一段情节中，一个状态只会出现一次，或者只需计算第一次的价值。
    - every-visit - 在一段情节中，一个状态可能会被访问多次，需要计算每一次的价值。
    - on-policy method - 评估和优化的策略和模拟的策略是同一个。
      - 存在局部最优问题，使用ε-greedy 解决
    - off-policy method - 评估和优化的策略和模拟的策略是不同的两个。有时候，模拟数据来源于其它处，比如：已有的数据，或者人工模拟等等。
      - target policy - 目标策略。off policy method中，需要优化的策略。
      - behavior policy - 行为策略。off policy method中，模拟数据来源的策略。
      - 优势
        - 更强的通用性，确保了数据全面性，所有行为都能覆盖
        - 保证了探索性，在选取next的时候，需要选取max的一个，覆盖了action 的所有q值，从而覆盖了所有行为
      - 劣势
        - 过高估计问题——解决方法：Double learning --->这在reinforcement learning an introduction 和 DDQN里面都有

  - ### 表格式方法

    - #### 马尔科夫性质

      - 当一个随机过程在给定现在状态及所有过去状态情况下，其未来状态的条件概率分布仅依赖于当前状态，与过去状态是条件独立的（即与历史路径没有关系）
      - 具有马尔可夫性质的过程通常称之为马尔可夫过程

    - #### 贝尔曼公式

      - 贝尔曼公式

        ![img](https://mubu.com/document_image/400b8c36-d64d-4e39-9694-8428b0b21dac-726627.jpg)

      - 贝尔曼值最优化方程 (for v*)

        ![img](https://mubu.com/document_image/c051e03d-01e3-4ab9-9fa0-d9d60f917cfb-726627.jpg)

      - 贝尔曼值最优方程(for q*)

        ![img](https://mubu.com/document_image/aeac0ff9-ad0f-4d38-b332-31eaf7771a82-726627.jpg)

    - #### DP

      - 通用迭代公式(Generalized Policy Iteration)

        - 主要思想：1.从一个策略π0开始，显示策略评估，得到策略π0的价值Vπ0，2.策略改善，根据价值Vπ0，优化策略为π0'，3.重复迭代前两个步骤，得到最优策略π*（终止条件π不变了，或者策略价值不变了）

        - Policy Iteration

          ![img](https://mubu.com/document_image/cd078fdb-69ed-4821-8148-ffe9bf835342-726627.jpg)

        - Value Iteration

          ![img](https://mubu.com/document_image/463f1fd3-549a-4121-94d9-ef5bfc116b33-726627.jpg)

      - 损失函数

    - #### 蒙特卡洛方法

      - important sample

        - 重要性采样<https://blog.csdn.net/wangpeng138375/article/details/74645637>

          ![img](https://mubu.com/document_image/2dff6a6d-bff8-4a21-865c-e7681e53403e-726627.jpg)

        - 在强化学习方面的应用

          ![img](https://mubu.com/document_image/c3cc1a09-c7e7-4b23-a366-0cf2e64d969c-726627.jpg)

          - ordinary importance sampling

            ![img](https://mubu.com/document_image/3f857075-6849-47de-9d63-f8fcc58e25af-726627.jpg)

            - unbiased & variance is in general unbounded

          - weighted importance sampling(use weighted average)

            ![img](https://mubu.com/document_image/3b03e313-e088-4f67-88ec-ccae59696dce-726627.jpg)

            - biased & usually has dramatically lower variance and is strongly preferred

      - first-visit

      - every-visit

      - 使用了weighted importance sampling 的 off-policy 蒙特卡洛

        ![img](https://mubu.com/document_image/9a1eb5a3-c124-4862-bdf8-7f971119a889-726627.jpg)

      - 对应代码<https://blog.csdn.net/u010223750/article/details/78884172>

      - 优缺点

        - 可以从交互中直接学习优化策略而不需要一个环境的动态模型
        - 缺点是需要大量探索，基于概率，是不确定的

    - #### TD

      - TD偏差

        ![img](https://mubu.com/document_image/f95f8962-6314-4b74-88bb-775b90dc0a0c-726627.jpg)

      - Q-learning

        ![img](https://mubu.com/document_image/04eeb622-88d4-4c65-a354-5d781c6f065c-726627.jpg)

      - SARSA

        ![img](https://mubu.com/document_image/28934b84-c949-4fc0-a789-3ea649a744ae-726627.jpg)

      - n-step sarsa

        ![img](https://mubu.com/document_image/72839615-0210-44f0-8037-29ed31a83fe9-726627.jpg)

      - 公式

        ![img](https://mubu.com/document_image/169512db-3011-46aa-83f8-46e46573d7fb-726627.jpg)

    - #### TD，蒙特卡罗，DP方法的区别

      - DP VS 蒙特卡洛
        - DP方法是需要知道所有可能的状态转移的概率分布，而蒙特卡洛基于抽样数据（前者model-based,后者model-free，是否model-based看它的G是否是公式算出来的）,蒙特卡洛只适用于有限步骤的情节性任务
      - TD VS 蒙特卡洛
        - 蒙特卡洛需要在情节结束后再来估计状态价值
        - TD在行动一步或者几步后根据新状态的价值来估计当前状态价值
        - 蒙特卡洛的期望就是值函数的定义，所以无偏，但是得到的G_t随机性大，方差无穷大
        - TD目标为R_{t+1}+gammaV(S_{t+1}),所以，根据V(S_{t+1})是否为估计值确定是否无偏，但他只用到了有限步骤的随机状态和动作，所以随机性更小，方差小
        - <https://zhuanlan.zhihu.com/p/25913410>
      - TD VS DP
        - 蒙特卡罗的方法使用的是值函数最原始的定义，该方法利用所有回报的累积和估计值函数。DP方法和TD方法则利用一步预测方法计算当前状态值函数。其共同点是利用了bootstrapping方法，不同的是，DP方法利用模型计算后继状态，而TD方法利用试验得到后继状态。

    - Q-learning 和 SARSA 的区别

      - 强化学习中sarsa算法是不是比q-learning算法收敛速度更慢？ - 余帅的回答 - 知乎 <https://www.zhihu.com/question/268461866/answer/355932427>
      - Q-learning是off-policy，sarsa是on-policy，expected sarsa因为使用了期望而非最后真正实行的结果，所以是off-line，但是他的方差比sarsa小
      - sarsa收敛慢，在收敛后依旧会有探索步骤
      - Sarsa选取的是一种保守的策略，他在更新Q值的时候已经为未来规划好了动作，对错误和死亡比较敏感。而Q-learning每次在更新的时候选取的是最大化Q的方向，而当下一个状态时，再重新选择动作，Q-learning是一种鲁莽、大胆、贪婪的算法，对于死亡和错误并不在乎。

    - #### Dyna

      - 算法描述

        ![img](https://mubu.com/document_image/8e4bdf72-77d7-4a0d-832a-f84ef271ebe8-726627.jpg)

      - 存在这些情况

        - 1.环境随机，而仅仅部分采样可以被观察到
        - 2.模型使用function approximation学习，可能产生的不正确
        - 3.环境变了，而有新的行为没有被观察到

    - #### Dyna+

      - 出现的原因：针对Dyna环境变了，而有新的行为没有被观察到的问题，Dyna+存储了station-action对 的上一次try的time steps，在sample experience from the model 时，将reward 改为 reward + k * 根号（time steps差），这会在update时产生影响
      - 对unstationary的环境适应的更快

    - #### prioritized sweeping

      - 属于一种性能的优化

      - 算法描述

        ![img](https://mubu.com/document_image/7ae3d34b-de93-4a5e-ad54-33f83441e93f-726627.jpg)

  - ### 函数近似

    - why 函数逼近，以及他的好处

      - 第一，对于复杂环境，状态和行动太多，需要大量的空间来记忆策略价值，同时也需要更多的数据和计算量
      - 第二，在许多目标任务中，遇到的状态可能从未出现过
      - 因此需要用一个通用的方法来计算策略价值

    - #### Policy Gradient Methods 

      - 和基于价值函数的区别

        - 选择策略的方式的区别

          - 基于价值函数时，π(s) = argmax_a vπ( s' | s , a ),或者，π(s) = argmax_a qπ(s,a)
          - 基于策略梯度方法时，π(s) = argmax_a π(a | s,θ)

        - 更新规则区别

          - value based

            - 

              ![img](https://mubu.com/document_image/540c846b-3b0d-4d78-8922-110ba25570f9-726627.jpg)

          - policy based

            - 如果ddiscounting系数γ 为1 ，那么公式如下，不然Gt前面还有内容

              ![img](https://mubu.com/document_image/289e4383-4717-4b11-96e9-4666d2d99ba3-726627.jpg)

        - 优缺点

          - value-based对于连续动作 不能对每个动作都给予一个Q值 因而在连续动作集合中就不能有很好的表现；policy-based可以有效地处理连续动作集的问题「值函数方法无法确定一个对应max Q的action」，在高维和连续动作空间更加有效，可以学习随机的策略，但同样的容易收敛到局部最小值、方差较大。
          - 如果不用Import Sample,policy gradient 每次更新完theta就要重新采样，因为更新完参数，他的policy分布发生变化，而他更新参数使用的数据是按照policy概率分布采样，既然分布变了，原来的数据就不能用了——on-policy

      - 推导过程

        - The distribution μ here (as in Chapters 9 and 10) is the on-policy distribution under π (see page 199).  J(θ)表示 一个策略所能得到的折扣的奖赏，从初始状态 s0 出发得到的所有的平均

          ![img](https://mubu.com/document_image/c20137a2-3133-46ef-a226-f9a3bed37c34-726627.jpg)

          ![img](https://mubu.com/document_image/6a59e43d-4255-4e0e-92e7-877d0b60ce04-726627.jpg)

      - with Baseline

        - 新的算法

          - 性能函数J(θ)

            ![img](https://mubu.com/document_image/91d353ff-a7cb-401d-9eeb-fe0e58ccaea9-726627.jpg)

          - 更新公式

            ![img](https://mubu.com/document_image/f3ecebb5-e40b-4615-b6a5-cb1fda4b062c-726627.jpg)

          - 具体算法过程

            - 

              ![img](https://mubu.com/document_image/73d1b296-85b1-413c-9a06-1ca5e04b43aa-726627.jpg)

        - 好处

          - 减少方差

        - baseline 要求

          - 任意函数，甚至是随机变量，但要求与动作无关，这样增加项的和为0

            ![img](https://mubu.com/document_image/96f4fbba-6472-4d28-8744-5eb4f924b2e6-726627.jpg)

      - Actor-Critic Methods

      - 针对连续性任务

        - 性能函数变为每个步骤的平均奖赏

          ![img](https://mubu.com/document_image/d560ddbc-779f-4fe1-b964-a44bd152e9c8-726627.jpg)

      - 无限个动作的连续空间下，策略参数化

        - 不学习每个动作概率，而是学习采取动作的概率分布——设定为分布在实数标量值得动作上的高斯分布，且均值和标准差依赖于动作和给的参数

          ![img](https://mubu.com/document_image/b341135b-161d-4b3a-988a-17323289594b-726627.jpg)

      - 参考资料

        - <https://www.cnblogs.com/steven-yang/p/6624253.html>
        - <https://blog.csdn.net/qq_25037903/article/details/82777802>

- ### 深度强化学习

  - #### value-based
    - DQN
    - double DQN
    - Prioritized Experience Replay
    - Dueling Network Architectures for Deep Reinforcement Learning
    - Distributional value function
    - Noisy DQN
    - RUDDER: Return Decomposition for Delayed Rewards
  - #### policy-based
    - Actor Critic
    - DDPG
    - A3C
    - DDPO
  - #### 信赖域
    - PPO
    - TRPO
    - <https://blog.csdn.net/weixin_41679411/article/details/82421121>

- ### 强化学习面试实战

  - 写Bellman方程
  - sarsa和qlearning区别 哪个谨慎 为什么
  - 介绍PPO，TRPO
  - DDPG中第二个D为什么要确定
  - A3C中的优势函数的意义
  - 如果问题不满足马尔科夫性怎么办，当前时刻的状态和它之前很多状态有关
    - 多个时刻状态并入考虑作为一个状态，或者使用RNN来学习其中隐含的时序信息；
  - DQN怎么设计 动作状态奖励 结构 loss  —— 以flip bird为例设计